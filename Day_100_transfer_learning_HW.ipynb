{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day_100_transfer_learning_HW",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPpuUxAHNjEy"
      },
      "source": [
        "## 作業\n",
        "礙於不是所有同學都有 GPU ，這邊的範例使用的是簡化版本的 ResNet，確保所有同學都能夠順利訓練!\n",
        "\n",
        "\n",
        "最後一天的作業請閱讀這篇非常詳盡的[文章](https://blog.gtwang.org/programming/keras-resnet-50-pre-trained-model-build-dogs-cats-image-classification-system/)，基本上已經涵蓋了所有訓練　CNN 常用的技巧，請使用所有學過的訓練技巧，盡可能地提高 Cifar-10 的 test data 準確率，截圖你最佳的結果並上傳來完成最後一次的作業吧!\n",
        "\n",
        "另外這些技巧在 Kaggle 上也會被許多人使用，更有人會開發一些新的技巧，例如使把預訓練在 ImageNet 上的模型當成 feature extractor 後，再拿擷取出的特徵重新訓練新的模型，這些技巧再進階的課程我們會在提到，有興趣的同學也可以[參考](https://www.kaggle.com/insaff/img-feature-extraction-with-pretrained-resnet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnzH4sjhNUOf"
      },
      "source": [
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten, Dropout, Dense\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "#gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "#tf.config.experimental.set_memory_growth(gpu_devices[0], True)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm7sJTt0N7Fp"
      },
      "source": [
        "# Training parameters\n",
        "\n",
        "# 資料路徑\n",
        "DATASET_PATH  = '/content'\n",
        "\n",
        "# 影像大小\n",
        "IMAGE_SIZE = (224, 224)\n",
        "\n",
        "# 影像類別數\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# 若 GPU 記憶體不足，可調降 batch size 或凍結更多層網路\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# Epoch 數\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "data_augmentation = True\n",
        "\n",
        "# Subtracting pixel mean improves accuracy\n",
        "subtract_pixel_mean = True"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw3QPuiVN9G3"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIyk075y4sfn",
        "outputId": "391cac77-b23d-455e-8ede-de20ddf87220"
      },
      "source": [
        "#initial input shape\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Normalize\n",
        "x_train = x_train/255\n",
        "x_test = x_test/255\n",
        "\n",
        "#one-hot coding label\n",
        "y_train =  keras.utils.to_categorical(y_train,NUM_CLASSES)\n",
        "y_test = keras.utils.to_categorical(y_test,NUM_CLASSES)\n",
        "\n",
        "if subtract_pixel_mean:\n",
        "  x_train_mean = np.mean(x_train)\n",
        "  x_test_mean = np.mean(x_test)\n",
        "  x_train = x_train - x_train_mean\n",
        "  x_test = x_test - x_test_mean\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "y_train shape: (50000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlyK2m30T0i-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eevji7Xj7pdl"
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    lr = 1e-3\n",
        "    if epoch > 40:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 30:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 20:\n",
        "        lr *= 1e-1\n",
        "    elif epoch > 10:\n",
        "        lr *= 3e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5IRdW2072x0"
      },
      "source": [
        "class Mycallback(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    print(K.eval(self.model.optimizer.lr))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzM96zPp-wII"
      },
      "source": [
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=\"/content\",\n",
        "                             monitor='val_accuracy',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=3,\n",
        "                               min_lr=1e-8)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler, Mycallback()]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFVYv7tj-3T3",
        "outputId": "d5afaa13-411f-4d3c-a8fb-ebeed29d2003"
      },
      "source": [
        "# initialize pre-trained model\n",
        "net = ResNet50(include_top=False, \n",
        "               weights='imagenet', \n",
        "               input_tensor=None,\n",
        "               input_shape=(32, 32, 3))\n",
        "\n",
        "\n",
        "#remove classification layer\n",
        "net.layers.pop()\n",
        "net = Model(inputs=net.input, outputs=net.layers[-1].output)\n",
        "\n",
        "for layer in net.layers[:-5]:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = net.output\n",
        "x = Flatten()(x)\n",
        "\n",
        "# 增加 DropOut layer\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "# 增加 Dense layer，以 softmax 產生個類別的機率值\n",
        "output_layer = Dense(NUM_CLASSES, activation='softmax', kernel_initializer='he_normal', name=\"classifier\")(x)\n",
        "\n",
        "net_final = Model(inputs=net.input, outputs=output_layer)\n",
        "\n",
        "# 使用 Adam optimizer，以較低的 learning rate 進行 fine-tuning\n",
        "net_final.compile(optimizer=RMSprop(learning_rate=lr_schedule(0)),\n",
        "                  loss='categorical_crossentropy', \n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# 輸出整個網路結構\n",
        "print(net_final.summary())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate:  0.001\n",
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 2048)         0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 2048)         0           flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 512)          1049088     dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 10)           5130        dense_4[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 24,641,930\n",
            "Trainable params: 2,108,938\n",
            "Non-trainable params: 22,532,992\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcsSfqvGN-ZM",
        "outputId": "4da80977-40d1-4321-e14c-d2f1961dfc4d"
      },
      "source": [
        "# Run training, with or without data augmentation.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    net_final.fit(x_train, y_train,\n",
        "              batch_size=BATCH_SIZE,\n",
        "              epochs=NUM_EPOCHS,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True,\n",
        "              callbacks=callbacks)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    # 透過 data augmentation 產生訓練與驗證用的影像資料\n",
        "    net_final.fit(x_train, y_train,\n",
        "              batch_size=BATCH_SIZE,\n",
        "              epochs=NUM_EPOCHS,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True,\n",
        "              callbacks=callbacks)\n",
        "    \n",
        "    train_datagen = ImageDataGenerator(rotation_range=20,\n",
        "                                       width_shift_range=0.1,\n",
        "                                       height_shift_range=0.1,\n",
        "                                       shear_range=0.1,\n",
        "                                       zoom_range=0.1,\n",
        "                                       channel_shift_range=0,\n",
        "                                       horizontal_flip=True,\n",
        "                                       fill_mode='nearest')\n",
        "    model_value_aug = net_final.fit_generator(train_datagen.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
        "                                  validation_data=(x_test, y_test), \n",
        "                                  steps_per_epoch=len(x_train) // BATCH_SIZE,\n",
        "                                  epochs=NUM_EPOCHS,\n",
        "                                  callbacks=callbacks)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 15s 31ms/step - loss: 2.3503 - accuracy: 0.2710 - val_loss: 5.0663 - val_accuracy: 0.1802\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.18020, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "0.001\n",
            "Epoch 2/50\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.7194 - accuracy: 0.3861 - val_loss: 13.2861 - val_accuracy: 0.1327\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.18020\n",
            "0.001\n",
            "Epoch 3/50\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.6502 - accuracy: 0.4158 - val_loss: 9.2066 - val_accuracy: 0.2430\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.18020 to 0.24300, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "0.001\n",
            "Epoch 4/50\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.5984 - accuracy: 0.4356 - val_loss: 18.7734 - val_accuracy: 0.1638\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.24300\n",
            "0.0003162278\n",
            "Epoch 5/50\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.5709 - accuracy: 0.4497 - val_loss: 28.5848 - val_accuracy: 0.1588\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.24300\n",
            "0.001\n",
            "Epoch 6/50\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.5436 - accuracy: 0.4589 - val_loss: 36.5816 - val_accuracy: 0.1851\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.24300\n",
            "0.001\n",
            "Epoch 7/50\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.5347 - accuracy: 0.4603 - val_loss: 40.5205 - val_accuracy: 0.1995\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.24300\n",
            "0.0003162278\n",
            "Epoch 8/50\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.5063 - accuracy: 0.4745 - val_loss: 45.3212 - val_accuracy: 0.1593\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.24300\n",
            "0.001\n",
            "Epoch 9/50\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.4967 - accuracy: 0.4748 - val_loss: 15.6006 - val_accuracy: 0.2699\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.24300 to 0.26990, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "0.001\n",
            "Epoch 10/50\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.4804 - accuracy: 0.4784 - val_loss: 53.8224 - val_accuracy: 0.2284\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.26990\n",
            "0.0003162278\n",
            "Epoch 11/50\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.4604 - accuracy: 0.4851 - val_loss: 50.0165 - val_accuracy: 0.1846\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.26990\n",
            "0.001\n",
            "Epoch 12/50\n",
            "Learning rate:  0.0003\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.4095 - accuracy: 0.5040 - val_loss: 2.1296 - val_accuracy: 0.3646\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.26990 to 0.36460, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "0.0003\n",
            "Epoch 13/50\n",
            "Learning rate:  0.0003\n",
            "391/391 [==============================] - 12s 29ms/step - loss: 1.3707 - accuracy: 0.5222 - val_loss: 1.9989 - val_accuracy: 0.3908\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.36460 to 0.39080, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "0.0003\n",
            "Epoch 14/50\n",
            "Learning rate:  0.0003\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.3614 - accuracy: 0.5200 - val_loss: 1.7992 - val_accuracy: 0.4153\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.39080 to 0.41530, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "0.0003\n",
            "Epoch 15/50\n",
            "Learning rate:  0.0003\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.3519 - accuracy: 0.5258 - val_loss: 4.8823 - val_accuracy: 0.3282\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.41530\n",
            "0.0003\n",
            "Epoch 16/50\n",
            "Learning rate:  0.0003\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.3434 - accuracy: 0.5302 - val_loss: 2.0735 - val_accuracy: 0.3971\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.41530\n",
            "0.0003\n",
            "Epoch 17/50\n",
            "Learning rate:  0.0003\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.3361 - accuracy: 0.5280 - val_loss: 2.1692 - val_accuracy: 0.4008\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.41530\n",
            "9.486834e-05\n",
            "Epoch 18/50\n",
            "Learning rate:  0.0003\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.3184 - accuracy: 0.5382 - val_loss: 2.5166 - val_accuracy: 0.4111\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.41530\n",
            "0.0003\n",
            "Epoch 19/50\n",
            "Learning rate:  0.0003\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.3126 - accuracy: 0.5410 - val_loss: 2.7424 - val_accuracy: 0.3992\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.41530\n",
            "0.0003\n",
            "Epoch 20/50\n",
            "Learning rate:  0.0003\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.3147 - accuracy: 0.5369 - val_loss: 3.1745 - val_accuracy: 0.3736\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.41530\n",
            "9.486834e-05\n",
            "Epoch 21/50\n",
            "Learning rate:  0.0003\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.3005 - accuracy: 0.5462 - val_loss: 3.6535 - val_accuracy: 0.3754\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.41530\n",
            "0.0003\n",
            "Epoch 22/50\n",
            "Learning rate:  0.0001\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.2840 - accuracy: 0.5516 - val_loss: 1.4428 - val_accuracy: 0.4978\n",
            "\n",
            "Epoch 00022: val_accuracy improved from 0.41530 to 0.49780, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "1e-04\n",
            "Epoch 23/50\n",
            "Learning rate:  0.0001\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.2669 - accuracy: 0.5541 - val_loss: 1.4745 - val_accuracy: 0.4889\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.49780\n",
            "1e-04\n",
            "Epoch 24/50\n",
            "Learning rate:  0.0001\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.2683 - accuracy: 0.5583 - val_loss: 1.4322 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 00024: val_accuracy improved from 0.49780 to 0.50250, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "1e-04\n",
            "Epoch 25/50\n",
            "Learning rate:  0.0001\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.2591 - accuracy: 0.5583 - val_loss: 1.4398 - val_accuracy: 0.5010\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.50250\n",
            "1e-04\n",
            "Epoch 26/50\n",
            "Learning rate:  0.0001\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.2539 - accuracy: 0.5581 - val_loss: 1.4391 - val_accuracy: 0.5029\n",
            "\n",
            "Epoch 00026: val_accuracy improved from 0.50250 to 0.50290, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "1e-04\n",
            "Epoch 27/50\n",
            "Learning rate:  0.0001\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.2502 - accuracy: 0.5605 - val_loss: 1.4638 - val_accuracy: 0.5020\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.50290\n",
            "3.1622774e-05\n",
            "Epoch 28/50\n",
            "Learning rate:  0.0001\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.2575 - accuracy: 0.5592 - val_loss: 1.4276 - val_accuracy: 0.5077\n",
            "\n",
            "Epoch 00028: val_accuracy improved from 0.50290 to 0.50770, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "1e-04\n",
            "Epoch 29/50\n",
            "Learning rate:  0.0001\n",
            "391/391 [==============================] - 12s 30ms/step - loss: 1.2449 - accuracy: 0.5630 - val_loss: 1.5617 - val_accuracy: 0.4806\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.50770\n",
            "1e-04\n",
            "Epoch 30/50\n",
            "Learning rate:  0.0001\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.2461 - accuracy: 0.5645 - val_loss: 1.4291 - val_accuracy: 0.5058\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.50770\n",
            "1e-04\n",
            "Epoch 31/50\n",
            "Learning rate:  0.0001\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.2506 - accuracy: 0.5656 - val_loss: 1.4170 - val_accuracy: 0.5078\n",
            "\n",
            "Epoch 00031: val_accuracy improved from 0.50770 to 0.50780, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "1e-04\n",
            "Epoch 32/50\n",
            "Learning rate:  1e-05\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.2400 - accuracy: 0.5613 - val_loss: 1.3546 - val_accuracy: 0.5312\n",
            "\n",
            "Epoch 00032: val_accuracy improved from 0.50780 to 0.53120, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "1e-05\n",
            "Epoch 33/50\n",
            "Learning rate:  1e-05\n",
            "391/391 [==============================] - 12s 29ms/step - loss: 1.2367 - accuracy: 0.5680 - val_loss: 1.3581 - val_accuracy: 0.5321\n",
            "\n",
            "Epoch 00033: val_accuracy improved from 0.53120 to 0.53210, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "1e-05\n",
            "Epoch 34/50\n",
            "Learning rate:  1e-05\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.2297 - accuracy: 0.5695 - val_loss: 1.3543 - val_accuracy: 0.5330\n",
            "\n",
            "Epoch 00034: val_accuracy improved from 0.53210 to 0.53300, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "1e-05\n",
            "Epoch 35/50\n",
            "Learning rate:  1e-05\n",
            "391/391 [==============================] - 12s 29ms/step - loss: 1.2253 - accuracy: 0.5704 - val_loss: 1.3544 - val_accuracy: 0.5325\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.53300\n",
            "1e-05\n",
            "Epoch 36/50\n",
            "Learning rate:  1e-05\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.2278 - accuracy: 0.5672 - val_loss: 1.3536 - val_accuracy: 0.5326\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.53300\n",
            "1e-05\n",
            "Epoch 37/50\n",
            "Learning rate:  1e-05\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.2282 - accuracy: 0.5693 - val_loss: 1.3541 - val_accuracy: 0.5304\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.53300\n",
            "1e-05\n",
            "Epoch 38/50\n",
            "Learning rate:  1e-05\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.2206 - accuracy: 0.5742 - val_loss: 1.3538 - val_accuracy: 0.5325\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.53300\n",
            "1e-05\n",
            "Epoch 39/50\n",
            "Learning rate:  1e-05\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.2223 - accuracy: 0.5748 - val_loss: 1.3544 - val_accuracy: 0.5330\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.53300\n",
            "3.1622776e-06\n",
            "Epoch 40/50\n",
            "Learning rate:  1e-05\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.2168 - accuracy: 0.5726 - val_loss: 1.3551 - val_accuracy: 0.5324\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.53300\n",
            "1e-05\n",
            "Epoch 41/50\n",
            "Learning rate:  1e-05\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.2305 - accuracy: 0.5695 - val_loss: 1.3538 - val_accuracy: 0.5339\n",
            "\n",
            "Epoch 00041: val_accuracy improved from 0.53300 to 0.53390, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "1e-05\n",
            "Epoch 42/50\n",
            "Learning rate:  1e-06\n",
            "391/391 [==============================] - 12s 29ms/step - loss: 1.2233 - accuracy: 0.5692 - val_loss: 1.3527 - val_accuracy: 0.5318\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.53390\n",
            "1e-06\n",
            "Epoch 43/50\n",
            "Learning rate:  1e-06\n",
            "391/391 [==============================] - 12s 30ms/step - loss: 1.2319 - accuracy: 0.5683 - val_loss: 1.3518 - val_accuracy: 0.5317\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.53390\n",
            "1e-06\n",
            "Epoch 44/50\n",
            "Learning rate:  1e-06\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.2156 - accuracy: 0.5722 - val_loss: 1.3521 - val_accuracy: 0.5313\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.53390\n",
            "1e-06\n",
            "Epoch 45/50\n",
            "Learning rate:  1e-06\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.2120 - accuracy: 0.5744 - val_loss: 1.3529 - val_accuracy: 0.5315\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.53390\n",
            "1e-06\n",
            "Epoch 46/50\n",
            "Learning rate:  1e-06\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.2188 - accuracy: 0.5682 - val_loss: 1.3526 - val_accuracy: 0.5319\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.53390\n",
            "3.1622776e-07\n",
            "Epoch 47/50\n",
            "Learning rate:  1e-06\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.2215 - accuracy: 0.5723 - val_loss: 1.3525 - val_accuracy: 0.5316\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.53390\n",
            "1e-06\n",
            "Epoch 48/50\n",
            "Learning rate:  1e-06\n",
            "391/391 [==============================] - 12s 30ms/step - loss: 1.2290 - accuracy: 0.5704 - val_loss: 1.3522 - val_accuracy: 0.5324\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.53390\n",
            "1e-06\n",
            "Epoch 49/50\n",
            "Learning rate:  1e-06\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.2207 - accuracy: 0.5707 - val_loss: 1.3524 - val_accuracy: 0.5328\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.53390\n",
            "3.1622776e-07\n",
            "Epoch 50/50\n",
            "Learning rate:  1e-06\n",
            "391/391 [==============================] - 11s 29ms/step - loss: 1.2163 - accuracy: 0.5728 - val_loss: 1.3527 - val_accuracy: 0.5322\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.53390\n",
            "1e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "Learning rate:  0.001\n",
            "390/390 [==============================] - 34s 82ms/step - loss: 1.6613 - accuracy: 0.4191 - val_loss: 231.2759 - val_accuracy: 0.1662\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.53390\n",
            "0.001\n",
            "Epoch 2/50\n",
            "Learning rate:  0.001\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.6574 - accuracy: 0.4166 - val_loss: 167.5126 - val_accuracy: 0.1716\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.53390\n",
            "0.001\n",
            "Epoch 3/50\n",
            "Learning rate:  0.001\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 1.6537 - accuracy: 0.4183 - val_loss: 133.5513 - val_accuracy: 0.1802\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.53390\n",
            "0.001\n",
            "Epoch 4/50\n",
            "Learning rate:  0.001\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.6367 - accuracy: 0.4237 - val_loss: 137.4284 - val_accuracy: 0.1564\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.53390\n",
            "0.001\n",
            "Epoch 5/50\n",
            "Learning rate:  0.001\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.6430 - accuracy: 0.4235 - val_loss: 146.3344 - val_accuracy: 0.1940\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.53390\n",
            "0.001\n",
            "Epoch 6/50\n",
            "Learning rate:  0.001\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 1.6393 - accuracy: 0.4238 - val_loss: 148.7952 - val_accuracy: 0.1551\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.53390\n",
            "0.0003162278\n",
            "Epoch 7/50\n",
            "Learning rate:  0.001\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.6287 - accuracy: 0.4299 - val_loss: 104.8456 - val_accuracy: 0.2051\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.53390\n",
            "0.001\n",
            "Epoch 8/50\n",
            "Learning rate:  0.001\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.6205 - accuracy: 0.4285 - val_loss: 178.3654 - val_accuracy: 0.1612\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.53390\n",
            "0.001\n",
            "Epoch 9/50\n",
            "Learning rate:  0.001\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 1.6231 - accuracy: 0.4282 - val_loss: 124.5694 - val_accuracy: 0.1728\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.53390\n",
            "0.001\n",
            "Epoch 10/50\n",
            "Learning rate:  0.001\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.6189 - accuracy: 0.4310 - val_loss: 109.3318 - val_accuracy: 0.1817\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.53390\n",
            "0.0003162278\n",
            "Epoch 11/50\n",
            "Learning rate:  0.001\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 1.6193 - accuracy: 0.4274 - val_loss: 110.8513 - val_accuracy: 0.1821\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.53390\n",
            "0.001\n",
            "Epoch 12/50\n",
            "Learning rate:  0.0003\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 1.5824 - accuracy: 0.4424 - val_loss: 1.9719 - val_accuracy: 0.3802\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.53390\n",
            "0.0003\n",
            "Epoch 13/50\n",
            "Learning rate:  0.0003\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.5674 - accuracy: 0.4496 - val_loss: 2.6195 - val_accuracy: 0.4242\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.53390\n",
            "0.0003\n",
            "Epoch 14/50\n",
            "Learning rate:  0.0003\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 1.5669 - accuracy: 0.4515 - val_loss: 1.9869 - val_accuracy: 0.4025\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.53390\n",
            "0.0003\n",
            "Epoch 15/50\n",
            "Learning rate:  0.0003\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.5577 - accuracy: 0.4522 - val_loss: 2.6548 - val_accuracy: 0.3983\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.53390\n",
            "9.486834e-05\n",
            "Epoch 16/50\n",
            "Learning rate:  0.0003\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 1.5534 - accuracy: 0.4549 - val_loss: 3.1073 - val_accuracy: 0.3798\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.53390\n",
            "0.0003\n",
            "Epoch 17/50\n",
            "Learning rate:  0.0003\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.5437 - accuracy: 0.4572 - val_loss: 1.6741 - val_accuracy: 0.4479\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.53390\n",
            "0.0003\n",
            "Epoch 18/50\n",
            "Learning rate:  0.0003\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.5420 - accuracy: 0.4562 - val_loss: 1.8789 - val_accuracy: 0.4254\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.53390\n",
            "0.0003\n",
            "Epoch 19/50\n",
            "Learning rate:  0.0003\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.5459 - accuracy: 0.4571 - val_loss: 3.3072 - val_accuracy: 0.3968\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.53390\n",
            "0.0003\n",
            "Epoch 20/50\n",
            "Learning rate:  0.0003\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 1.5443 - accuracy: 0.4592 - val_loss: 2.2067 - val_accuracy: 0.4371\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.53390\n",
            "9.486834e-05\n",
            "Epoch 21/50\n",
            "Learning rate:  0.0003\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 1.5460 - accuracy: 0.4586 - val_loss: 3.1247 - val_accuracy: 0.3672\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.53390\n",
            "0.0003\n",
            "Epoch 22/50\n",
            "Learning rate:  0.0001\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 1.5277 - accuracy: 0.4618 - val_loss: 1.4296 - val_accuracy: 0.5001\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.53390\n",
            "1e-04\n",
            "Epoch 23/50\n",
            "Learning rate:  0.0001\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.5214 - accuracy: 0.4687 - val_loss: 1.4323 - val_accuracy: 0.4963\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.53390\n",
            "1e-04\n",
            "Epoch 24/50\n",
            "Learning rate:  0.0001\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 1.5258 - accuracy: 0.4658 - val_loss: 1.4647 - val_accuracy: 0.4858\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.53390\n",
            "1e-04\n",
            "Epoch 25/50\n",
            "Learning rate:  0.0001\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.5213 - accuracy: 0.4691 - val_loss: 1.4167 - val_accuracy: 0.5050\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.53390\n",
            "1e-04\n",
            "Epoch 26/50\n",
            "Learning rate:  0.0001\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.5273 - accuracy: 0.4646 - val_loss: 1.4270 - val_accuracy: 0.5006\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.53390\n",
            "1e-04\n",
            "Epoch 27/50\n",
            "Learning rate:  0.0001\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.5247 - accuracy: 0.4631 - val_loss: 1.3694 - val_accuracy: 0.5265\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.53390\n",
            "1e-04\n",
            "Epoch 28/50\n",
            "Learning rate:  0.0001\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.5152 - accuracy: 0.4698 - val_loss: 1.4000 - val_accuracy: 0.5096\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.53390\n",
            "1e-04\n",
            "Epoch 29/50\n",
            "Learning rate:  0.0001\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.5153 - accuracy: 0.4695 - val_loss: 1.4494 - val_accuracy: 0.4939\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.53390\n",
            "1e-04\n",
            "Epoch 30/50\n",
            "Learning rate:  0.0001\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.5158 - accuracy: 0.4673 - val_loss: 1.4244 - val_accuracy: 0.5053\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.53390\n",
            "3.1622774e-05\n",
            "Epoch 31/50\n",
            "Learning rate:  0.0001\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.5152 - accuracy: 0.4682 - val_loss: 1.4604 - val_accuracy: 0.4923\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.53390\n",
            "1e-04\n",
            "Epoch 32/50\n",
            "Learning rate:  1e-05\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.5143 - accuracy: 0.4697 - val_loss: 1.3473 - val_accuracy: 0.5317\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.53390\n",
            "1e-05\n",
            "Epoch 33/50\n",
            "Learning rate:  1e-05\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.5155 - accuracy: 0.4694 - val_loss: 1.3519 - val_accuracy: 0.5300\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.53390\n",
            "1e-05\n",
            "Epoch 34/50\n",
            "Learning rate:  1e-05\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.5121 - accuracy: 0.4733 - val_loss: 1.3483 - val_accuracy: 0.5338\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.53390\n",
            "1e-05\n",
            "Epoch 35/50\n",
            "Learning rate:  1e-05\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.5121 - accuracy: 0.4714 - val_loss: 1.3468 - val_accuracy: 0.5314\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.53390\n",
            "1e-05\n",
            "Epoch 36/50\n",
            "Learning rate:  1e-05\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.5083 - accuracy: 0.4705 - val_loss: 1.3467 - val_accuracy: 0.5334\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.53390\n",
            "1e-05\n",
            "Epoch 37/50\n",
            "Learning rate:  1e-05\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.5100 - accuracy: 0.4675 - val_loss: 1.3502 - val_accuracy: 0.5311\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.53390\n",
            "1e-05\n",
            "Epoch 38/50\n",
            "Learning rate:  1e-05\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.5024 - accuracy: 0.4722 - val_loss: 1.3448 - val_accuracy: 0.5316\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.53390\n",
            "1e-05\n",
            "Epoch 39/50\n",
            "Learning rate:  1e-05\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.5058 - accuracy: 0.4723 - val_loss: 1.3487 - val_accuracy: 0.5321\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.53390\n",
            "1e-05\n",
            "Epoch 40/50\n",
            "Learning rate:  1e-05\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.5070 - accuracy: 0.4720 - val_loss: 1.3446 - val_accuracy: 0.5321\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.53390\n",
            "1e-05\n",
            "Epoch 41/50\n",
            "Learning rate:  1e-05\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.5050 - accuracy: 0.4722 - val_loss: 1.3429 - val_accuracy: 0.5322\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.53390\n",
            "1e-05\n",
            "Epoch 42/50\n",
            "Learning rate:  1e-06\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.5049 - accuracy: 0.4726 - val_loss: 1.3448 - val_accuracy: 0.5336\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.53390\n",
            "1e-06\n",
            "Epoch 43/50\n",
            "Learning rate:  1e-06\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.5085 - accuracy: 0.4733 - val_loss: 1.3452 - val_accuracy: 0.5343\n",
            "\n",
            "Epoch 00043: val_accuracy improved from 0.53390 to 0.53430, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "1e-06\n",
            "Epoch 44/50\n",
            "Learning rate:  1e-06\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.5078 - accuracy: 0.4707 - val_loss: 1.3447 - val_accuracy: 0.5344\n",
            "\n",
            "Epoch 00044: val_accuracy improved from 0.53430 to 0.53440, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "3.1622776e-07\n",
            "Epoch 45/50\n",
            "Learning rate:  1e-06\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.5031 - accuracy: 0.4736 - val_loss: 1.3452 - val_accuracy: 0.5345\n",
            "\n",
            "Epoch 00045: val_accuracy improved from 0.53440 to 0.53450, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "1e-06\n",
            "Epoch 46/50\n",
            "Learning rate:  1e-06\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.5072 - accuracy: 0.4737 - val_loss: 1.3459 - val_accuracy: 0.5336\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.53450\n",
            "1e-06\n",
            "Epoch 47/50\n",
            "Learning rate:  1e-06\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.5113 - accuracy: 0.4699 - val_loss: 1.3451 - val_accuracy: 0.5342\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.53450\n",
            "3.1622776e-07\n",
            "Epoch 48/50\n",
            "Learning rate:  1e-06\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.5076 - accuracy: 0.4721 - val_loss: 1.3449 - val_accuracy: 0.5340\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.53450\n",
            "1e-06\n",
            "Epoch 49/50\n",
            "Learning rate:  1e-06\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.5070 - accuracy: 0.4705 - val_loss: 1.3449 - val_accuracy: 0.5340\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.53450\n",
            "1e-06\n",
            "Epoch 50/50\n",
            "Learning rate:  1e-06\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.5095 - accuracy: 0.4711 - val_loss: 1.3453 - val_accuracy: 0.5342\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.53450\n",
            "3.1622776e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ueti-TO2Pety",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1fe1393-0a0a-4d2a-917e-feea1b3a7877"
      },
      "source": [
        "# Score trained model.\n",
        "scores = net_final.evaluate(x_test,y_test,verbose=1)\n",
        "print('Test loss:',scores[0])\n",
        "print('Test accuracy:',scores[1])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 4s 12ms/step - loss: 1.3453 - accuracy: 0.5342\n",
            "Test loss: 1.3452526330947876\n",
            "Test accuracy: 0.5342000126838684\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}